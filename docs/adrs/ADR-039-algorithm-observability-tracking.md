# ADR-039: Algorithm Observability & Adaptive Weight Preparation

**Status:** üìù Proposed
**Date:** 2025-12-08
**Depends on:** ADR-038 (Scoring Algorithms Reference), ADR-008 (Adaptive Thresholds)

## Context

Nos algorithmes de scoring (ADR-038) utilisent d√©sormais des formules multiplicatives et des m√©thodes avanc√©es (Spectral Clustering). Bien que plus robustes th√©oriquement, ces choix introduisent de nouveaux hyper-param√®tres (Bonus Reliability, Cluster Boost) qu'il faut valider empiriquement.

**Objectif:** Logger les signaux, les d√©cisions et les outcomes pour:

1.  Valider la pertinence du Spectral Clustering (est-ce que le cluster boost aide vraiment ?).
2.  Ajuster les facteurs multiplicatifs (Reliability Factor).
3.  Entra√Æner de futurs mod√®les de r√©gression (Learning to Rank).

## Decision

### 1. Structure de Logging Unifi√©e

Apr√®s chaque suggestion ou recherche, nous loguons un `AlgorithmTraceRecord` structur√© pour capturer tout le contexte de d√©cision.

```typescript
interface AlgorithmTraceRecord {
  // --- Contexte ---
  trace_id: string;
  timestamp: Date;
  intent?: string; // Si Active Search
  context_hash: string; // Lien vers AdaptiveThresholds

  // --- Mode & Cible ---
  algorithm_mode: "active_search" | "passive_suggestion";
  target_type: "tool" | "capability";

  // --- Signaux d'entr√©e (Raw Inputs) ---
  signals: {
    semantic_score?: number;
    tools_overlap?: number;
    success_rate?: number;
    pagerank?: number;
    cooccurrence?: number;

    // Graph Specifics
    graph_density: number;
    spectral_cluster_match: boolean; // Est-ce dans le m√™me cluster ?
    adamic_adar?: number;
  };

  // --- Param√®tres de l'Algo (Configuration) ---
  params: {
    alpha: number; // Semantic vs Graph balance
    reliability_factor: number; // Impact du success_rate
    structural_boost: number; // Impact du cluster match
  };

  // --- R√©sultat Calcul√© ---
  scores: {
    relevance_score: number; // Score de base
    final_score: number; // Score final apr√®s boosts
    threshold_used: number; // Seuil Adaptive utilis√©
  };

  // --- D√©cision ---
  decision: "accepted" | "rejected_by_threshold" | "filtered_by_reliability";

  // --- Outcome (Asynchrone / Update plus tard) ---
  outcome?: {
    user_action: "selected" | "ignored" | "explicit_rejection";
    execution_success?: boolean;
    duration_ms?: number;
  };
}
```

### 2. Stockage

**Phase 1 (MVP):** Table `algorithm_traces` dans PGlite.

- Retention: 7 jours (c'est du debug/tuning, pas de l'audit log long terme).
- Colonnes cl√©s typ√©es, le reste en JSONB pour flexibilit√©.

```sql
CREATE TABLE algorithm_traces (
  trace_id UUID PRIMARY KEY,
  timestamp TIMESTAMPTZ DEFAULT NOW(),
  mode TEXT, -- 'active_search', 'passive_suggestion'
  target_type TEXT, -- 'tool', 'capability'
  final_score REAL,
  threshold_used REAL,
  decision TEXT,
  outcome TEXT, -- 'selected', 'ignored'
  details JSONB -- Tout le reste (signals, params...)
);
```

### 3. M√©triques Cl√©s √† Surveiller

Gr√¢ce √† ces logs, nous pourrons monitorer via un dashboard (Epic 6):

1.  **Spectral Relevance:** `AVG(outcome.selected WHERE signals.spectral_cluster_match = TRUE)`
    - _Question:_ Est-ce que les suggestions bas√©es sur le cluster sont plus cliqu√©es ?
2.  **Reliability Impact:** `CORRELATION(signals.success_rate, outcome.execution_success)`

    - _Question:_ Est-ce que notre p√©nalit√© sur le success_rate prot√®ge vraiment l'utilisateur ?

3.  **Threshold Efficiency:** `COUNT(rejected_by_threshold)` vs `COUNT(ignored)`
    - _Question:_ Est-ce qu'on rejette trop de choses ? Ou est-ce qu'on laisse passer trop de bruit ?

## Implementation Plan

1.  Cr√©er la migration `algorithm_traces`.
2.  Injecter un `AlgorithmTracer` dans `DAGSuggester` et `CapabilityMatcher`.
3.  Appeler `tracer.log(...)` √† la fin de chaque calcul de score.
4.  Ajouter une route API pour que le frontend puisse marquer une suggestion comme "selected" / "ignored".
